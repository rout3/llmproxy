proxy_configuration:
  route_type: interest

optional_configuration:
  timeout: 10 # Timeout for request to models
  force_timeout: true # WARNING: This can cause additonal costs!

provider_settings:
  - provider: OpenAI
    api_key_var: OPENAI_API_KEY # .env name for api key
    max_output_tokens: 256
    temperature: 0.1
    models: # input names of models you want to you (see below for all models provided)
      # Only models specified here will be added to routing pool
      - gpt-3.5-turbo-1106
      - gpt-4
      - gpt-4-32k
    # context: "Answer politely"
  #
  - provider: Llama2
    temperature: 0.1
    api_key_var: HUGGING_FACE_API_KEY
    max_output_tokens: 256
    models:
      - Llama-2-7b-chat-hf
      - Llama-2-13b-chat-hf
      - Llama-2-70b-chat-hf
      - Llama-2-7b
      - Llama-2-13b-chat-hf
      - Llama-2-13b-chat
      - Llama-2-13b-hf
      - Llama-2-13b
      - Llama-2-70b-chat-hf
      - Llama-2-70b-chat
      - Llama-2-70b-hf
      - Llama-2-70b

  - provider: Cohere
    temperature: 0.1
    api_key_var: COHERE_API_KEY
    max_output_tokens: 256
    models:
      - command
      - command-light
      - command-nightly
      - command-light-nightly

  - provider: Mistral
    temperature: 0.1
    api_key_var: MISTRAL_API_KEY
    max_output_tokens: 256
    models:
      - open-mixtral-8x7b
# DONT USE FOR TESTING FOR NOW TO AVOID CHARGES
# - provider: VertexAI
#   temperature: 0.1
#   api_key_var: GOOGLE_PROJECT_ID
#   max_output_tokens: 256
#   models:
#     - text-bison
# Currently not working
# - chat-bison